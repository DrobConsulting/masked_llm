{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drob7\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# 1) A discrete diffusion schedule\n",
    "# ------------------------------------------------------------------------\n",
    "class DiscreteDiffusionSchedule:\n",
    "    \"\"\"\n",
    "    Simple linear schedule of alpha_t from t=1..T,\n",
    "    where alpha_t = min_alpha + (max_alpha - min_alpha)*(t/T).\n",
    "    \"\"\"\n",
    "    def __init__(self, T=10, min_alpha=0.1, max_alpha=0.7):\n",
    "        self.T = T\n",
    "        self.alphas = []\n",
    "        for t in range(1, T+1):\n",
    "            frac = t / T\n",
    "            alpha_t = min_alpha + (max_alpha - min_alpha)*frac\n",
    "            self.alphas.append(alpha_t)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.T\n",
    "\n",
    "    def __getitem__(self, t):\n",
    "        # t in [1..T], python indexing 0..T-1\n",
    "        return self.alphas[t-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# 2) Forward noising that respects puzzle givens\n",
    "# ------------------------------------------------------------------------\n",
    "def forward_diffusion_with_puzzle(puzzle, solution, t, schedule, vocab_size, device):\n",
    "    \"\"\"\n",
    "    puzzle:   (batch, 81) with digits in [0..9]. 0 means blank, non-zero means given.\n",
    "    solution: (batch, 81) correct final solution\n",
    "    t:        an integer in [1..T]\n",
    "    schedule: contains alpha_t\n",
    "    returns x_t: partially noised solution (batch, 81)\n",
    "        - givens remain the same as solution's corresponding digit\n",
    "        - blank positions get replaced with random digits w.p. alpha_t\n",
    "    \"\"\"\n",
    "    alpha_t = schedule[t]  # fraction to noise\n",
    "    puzzle = puzzle.to(device)\n",
    "    solution = solution.to(device)\n",
    "\n",
    "    # Where puzzle is nonzero => givens => do NOT overwrite\n",
    "    givens_mask = (puzzle != 0)\n",
    "\n",
    "    # We'll noise only the positions that are blank in the puzzle\n",
    "    #   i.e. puzzle[i] == 0 => we can noise solution[i].\n",
    "    blank_mask = (puzzle == 0)\n",
    "\n",
    "    # Create random noise from [0..vocab_size-1] for the blank positions\n",
    "    noise = torch.randint(0, vocab_size, solution.shape, device=device)\n",
    "\n",
    "    # Decide which blank positions to replace with noise\n",
    "    replace_mask = (torch.rand_like(solution.float()) < alpha_t) & blank_mask\n",
    "\n",
    "    # x_t: start from the true solution, then replace with noise for some blank cells\n",
    "    x_t = solution.clone()\n",
    "    x_t[replace_mask] = noise[replace_mask]\n",
    "\n",
    "    # givens remain the same as the correct solution digit at that position\n",
    "    # (actually, this is already the default if puzzle != 0, but we do not overwrite them)\n",
    "    # so x_t[givens_mask] = solution[givens_mask] # if you want to be explicit\n",
    "\n",
    "    return x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# 3) The model sees puzzle+partially noised solution as input\n",
    "# ------------------------------------------------------------------------\n",
    "class PuzzleDenoiser(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer that takes [puzzle || x_t] => shape (batch, 162) tokens\n",
    "    and outputs a distribution over solution tokens for the second half.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.output_layer = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, puzzle, x_t):\n",
    "        \"\"\"\n",
    "        puzzle, x_t: (batch, 81)\n",
    "        We'll embed => shape (batch, 162, embed_dim)\n",
    "        Return shape => (batch, 162, vocab_size),\n",
    "        but we'll only evaluate the second half in the loss.\n",
    "        \"\"\"\n",
    "        batch_size = puzzle.size(0)\n",
    "        # Concatenate\n",
    "        inp = torch.cat([puzzle, x_t], dim=1)  # (batch, 162)\n",
    "        emb = self.embedding(inp)             # => (batch, 162, embed_dim)\n",
    "        enc_out = self.encoder(emb)           # => (batch, 162, embed_dim)\n",
    "        logits = self.output_layer(enc_out)   # => (batch, 162, vocab_size)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# 4) Diffusion train step\n",
    "# ------------------------------------------------------------------------\n",
    "def diffusion_train_step(\n",
    "    model, \n",
    "    puzzle,      # (batch, 81)\n",
    "    solution,    # (batch, 81)\n",
    "    schedule,\n",
    "    optimizer,\n",
    "    loss_fn,\n",
    "    vocab_size,\n",
    "    device\n",
    "):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Move to device\n",
    "    puzzle = puzzle.to(device)\n",
    "    solution = solution.to(device)\n",
    "\n",
    "    # Sample a random t\n",
    "    T = schedule.T\n",
    "    t = np.random.randint(1, T+1)\n",
    "\n",
    "    # Create x_t that doesn't overwrite puzzle givens\n",
    "    x_t = forward_diffusion_with_puzzle(\n",
    "        puzzle, solution, t, schedule, vocab_size, device\n",
    "    )\n",
    "\n",
    "    # Forward pass [puzzle || x_t]\n",
    "    logits = model(puzzle, x_t)  # shape (batch, 162, vocab_size)\n",
    "\n",
    "    # We only care about the second half (positions 81..161) \n",
    "    # for the solution. The first half is puzzle context.\n",
    "    logits_solution_part = logits[:, 81:, :]  # (batch, 81, vocab_size)\n",
    "\n",
    "    # Compare to ground truth solution\n",
    "    loss = loss_fn(\n",
    "        logits_solution_part.reshape(-1, vocab_size),\n",
    "        solution.reshape(-1)\n",
    "    )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# 5) Validation step\n",
    "# ------------------------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def diffusion_eval_step(\n",
    "    model, \n",
    "    puzzle,\n",
    "    solution,\n",
    "    schedule,\n",
    "    loss_fn,\n",
    "    vocab_size,\n",
    "    device\n",
    "):\n",
    "    model.eval()\n",
    "    puzzle = puzzle.to(device)\n",
    "    solution = solution.to(device)\n",
    "\n",
    "    t = np.random.randint(1, schedule.T+1)\n",
    "    x_t = forward_diffusion_with_puzzle(\n",
    "        puzzle, solution, t, schedule, vocab_size, device\n",
    "    )\n",
    "\n",
    "    logits = model(puzzle, x_t)\n",
    "    logits_solution_part = logits[:, 81:, :]  # only second half\n",
    "    loss = loss_fn(\n",
    "        logits_solution_part.reshape(-1, vocab_size),\n",
    "        solution.reshape(-1)\n",
    "    )\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# 6) Iterative decoding to fill blank cells\n",
    "# ------------------------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def iterative_decode(\n",
    "    model,\n",
    "    puzzle,        # shape (batch, 81) puzzle givens\n",
    "    schedule,\n",
    "    vocab_size,\n",
    "    device,\n",
    "    steps=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Steps:\n",
    "      1) Start from x_T = fully noised in blank cells (or partial).\n",
    "      2) For t in [T..1], model predicts solution => we partially adopt it in blank cells\n",
    "         respecting puzzle givens.\n",
    "      3) Return final x_0\n",
    "    If steps=None => run the full T steps from schedule.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    puzzle = puzzle.to(device)\n",
    "    batch_size, seq_len = puzzle.shape\n",
    "    T = schedule.T if steps is None else steps\n",
    "\n",
    "    # Initialize x_t: fully noised in blank positions, puzzle givens remain correct solution digits \n",
    "    # (We don't have the solution during inference, so let's use puzzle for givens \n",
    "    #  and random for blanks)\n",
    "    x_t = puzzle.clone()\n",
    "    blank_mask = (puzzle == 0)\n",
    "    # fill blanks with random\n",
    "    x_t[blank_mask] = torch.randint(0, vocab_size, x_t[blank_mask].shape, device=device)\n",
    "\n",
    "    for t in range(T, 0, -1):\n",
    "        # forward pass\n",
    "        logits = model(puzzle, x_t)         # (batch, 162, vocab_size)\n",
    "        logits_solution_part = logits[:, 81:, :]  # (batch, 81, vocab_size)\n",
    "\n",
    "        # predicted solution tokens for second half\n",
    "        pred_sol = logits_solution_part.argmax(dim=-1)  # (batch, 81)\n",
    "\n",
    "        # Now adopt the model's predictions *only* in blank cells.\n",
    "        # Keep puzzle givens as-is. But if puzzle[i] was 0, we update from pred_sol.\n",
    "        x_t[blank_mask] = pred_sol[blank_mask]\n",
    "\n",
    "        # Optionally, you can do partial or probabilistic \"denoising\" \n",
    "        # (like in the paper). For simplicity, we do a 1-step \"take argmax\" each iteration.\n",
    "\n",
    "    return x_t  # hopefully a filled solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# 7) Putting it all together: example training loop\n",
    "# ------------------------------------------------------------------------\n",
    "def train_puzzle_diffusion(\n",
    "    X_train, y_train, \n",
    "    X_val,   y_val,\n",
    "    vocab_size=10,\n",
    "    T=10,\n",
    "    embed_dim=256,\n",
    "    num_heads=8,\n",
    "    num_layers=4,\n",
    "    batch_size=32,\n",
    "    num_epochs=5\n",
    "):\n",
    "    \"\"\"\n",
    "    X_* are puzzle arrays of shape (N, 81).\n",
    "    y_* are solution arrays of shape (N, 81).\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    schedule = DiscreteDiffusionSchedule(\n",
    "        T=T, \n",
    "        min_alpha=0.1, \n",
    "        max_alpha=0.7\n",
    "    )\n",
    "\n",
    "    model = PuzzleDenoiser(\n",
    "        vocab_size=vocab_size, \n",
    "        embed_dim=embed_dim, \n",
    "        num_heads=num_heads, \n",
    "        num_layers=num_layers\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Setup data loaders\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset   = TensorDataset(X_val,   y_val)\n",
    "    train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader    = DataLoader(val_dataset,   batch_size=batch_size)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch_puzzle, batch_solution in tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\"):\n",
    "            loss_val = diffusion_train_step(\n",
    "                model=model,\n",
    "                puzzle=batch_puzzle,\n",
    "                solution=batch_solution,\n",
    "                schedule=schedule,\n",
    "                optimizer=optimizer,\n",
    "                loss_fn=loss_fn,\n",
    "                vocab_size=vocab_size,\n",
    "                device=device\n",
    "            )\n",
    "            train_losses.append(loss_val)\n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "\n",
    "        # Validation\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch_puzzle, batch_solution in tqdm(val_loader, desc=f\"Epoch {epoch+1} [Val  ]\"):\n",
    "                vloss = diffusion_eval_step(\n",
    "                    model=model,\n",
    "                    puzzle=batch_puzzle,\n",
    "                    solution=batch_solution,\n",
    "                    schedule=schedule,\n",
    "                    loss_fn=loss_fn,\n",
    "                    vocab_size=vocab_size,\n",
    "                    device=device\n",
    "                )\n",
    "                val_losses.append(vloss)\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: train_loss={avg_train_loss:.4f}, val_loss={avg_val_loss:.4f}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), \"puzzle_diffuser_best.pt\")\n",
    "            print(f\"  [*] Best model saved @ val_loss={avg_val_loss:.4f}\")\n",
    "\n",
    "    return model, schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load sudoku data\n",
    "df = pd.read_csv('./data/sudoku.csv')\n",
    "\n",
    "# Convert strings to tensors\n",
    "def preprocess_sudoku(puzzle_str):\n",
    "    # Convert string to list of integers and then to tensor\n",
    "    return torch.tensor([int(d) for d in puzzle_str], dtype=torch.long)\n",
    "\n",
    "# Convert all puzzles and solutions\n",
    "puzzles = torch.stack([preprocess_sudoku(p) for p in df['quizzes']])\n",
    "solutions = torch.stack([preprocess_sudoku(s) for s in df['solutions']])\n",
    "\n",
    "# Karpathy split (90/5/5)\n",
    "train_size = 0.9\n",
    "val_size = 0.05\n",
    "test_size = 0.05\n",
    "\n",
    "# First split into train and temp\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    puzzles, solutions, train_size=train_size, random_state=42\n",
    ")\n",
    "\n",
    "# Split temp into val and test\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]:   0%|          | 0/28125 [00:00<?, ?it/s]C:\\Users\\drob7\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "Epoch 1 [Train]:   1%|          | 220/28125 [00:08<17:04, 27.25it/s]"
     ]
    }
   ],
   "source": [
    "model, schedule = train_puzzle_diffusion(\n",
    "    X_train, y_train,\n",
    "    X_val,   y_val,\n",
    "    vocab_size=10,\n",
    "    T=10,\n",
    "    embed_dim=256,\n",
    "    num_heads=8,\n",
    "    num_layers=4,\n",
    "    batch_size=32,\n",
    "    num_epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
