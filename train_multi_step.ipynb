{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# 1) A discrete and Sigmoid diffusion schedule\n",
    "# ------------------------------------------------------------------------\n",
    "class DiscreteDiffusionSchedule:\n",
    "    \"\"\"\n",
    "    Simple linear schedule of alpha_t from t=1..T,\n",
    "    where alpha_t = min_alpha + (max_alpha - min_alpha)*(t/T).\n",
    "    \"\"\"\n",
    "    def __init__(self, T=10, min_alpha=0.1, max_alpha=0.7):\n",
    "        self.T = T\n",
    "        self.alphas = []\n",
    "        for t in range(1, T+1):\n",
    "            frac = t / T\n",
    "            alpha_t = min_alpha + (max_alpha - min_alpha)*frac\n",
    "            self.alphas.append(alpha_t)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.T\n",
    "\n",
    "    def __getitem__(self, t):\n",
    "        # t in [1..T], python indexing 0..T-1\n",
    "        return self.alphas[t-1]\n",
    "\n",
    "class SigmoidDiffusionSchedule:\n",
    "    \"\"\"\n",
    "    Sigmoid schedule of alpha_t from t=1..T.\n",
    "\n",
    "    alpha_t = min_alpha + (max_alpha - min_alpha)*sigmoid(k*(frac - 0.5)),\n",
    "    where frac = (t-1)/(T-1).\n",
    "    \"\"\"\n",
    "    def __init__(self, T=30, min_alpha=0.1, max_alpha=0.7, k=12.0):\n",
    "        self.T = T\n",
    "        self.alphas = []\n",
    "        for t in range(1, T+1):\n",
    "            # frac in [0..1]\n",
    "            frac = (t - 1) / (T - 1)  \n",
    "            # logistic\n",
    "            s = 1 / (1 + math.exp(-k * (frac - 0.5)))\n",
    "            alpha_t = min_alpha + (max_alpha - min_alpha) * s\n",
    "            self.alphas.append(alpha_t)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.T\n",
    "\n",
    "    def __getitem__(self, t):\n",
    "        # t in [1..T], python indexing 0..T-1\n",
    "        return self.alphas[t - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# 2) Forward noising that respects puzzle givens\n",
    "# ------------------------------------------------------------------------\n",
    "def forward_diffusion_with_puzzle(puzzle, solution, t, schedule, vocab_size, device):\n",
    "    \"\"\"\n",
    "    puzzle:   (batch, 81) with digits in [0..9]. 0 means blank, non-zero means given.\n",
    "    solution: (batch, 81) correct final solution\n",
    "    t:        an integer in [1..T]\n",
    "    schedule: contains alpha_t\n",
    "    returns x_t: partially noised solution (batch, 81)\n",
    "        - givens remain the same as solution's corresponding digit\n",
    "        - blank positions get replaced with random digits w.p. alpha_t\n",
    "    \"\"\"\n",
    "    alpha_t = schedule[t]  # fraction to noise\n",
    "    puzzle = puzzle.to(device)\n",
    "    solution = solution.to(device)\n",
    "\n",
    "    # Where puzzle is nonzero => givens => do NOT overwrite\n",
    "    givens_mask = (puzzle != 0)\n",
    "\n",
    "    # We'll noise only the positions that are blank in the puzzle\n",
    "    #   i.e. puzzle[i] == 0 => we can noise solution[i].\n",
    "    blank_mask = (puzzle == 0)\n",
    "\n",
    "    # Create random noise from [0..vocab_size-1] for the blank positions\n",
    "    noise = torch.randint(0, vocab_size, solution.shape, device=device)\n",
    "\n",
    "    # Decide which blank positions to replace with noise\n",
    "    replace_mask = (torch.rand_like(solution.float()) < alpha_t) & blank_mask\n",
    "\n",
    "    # x_t: start from the true solution, then replace with noise for some blank cells\n",
    "    x_t = solution.clone()\n",
    "    x_t[replace_mask] = noise[replace_mask]\n",
    "\n",
    "    # givens remain the same as the correct solution digit at that position\n",
    "    # (actually, this is already the default if puzzle != 0, but we do not overwrite them)\n",
    "    # so x_t[givens_mask] = solution[givens_mask] # if you want to be explicit\n",
    "\n",
    "    return x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# 3) The model sees puzzle+partially noised solution as input\n",
    "# ------------------------------------------------------------------------\n",
    "class PuzzleDenoiser(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, max_T=40):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Standard embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # 1) Add a time embedding for t in [1..max_T]\n",
    "        self.time_embedding = nn.Embedding(max_T + 1, embed_dim)\n",
    "        \n",
    "        # Positional embedding for puzzle+solution sequence\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, 163, embed_dim))\n",
    "        \n",
    "        # Layer norm after positional embeddings\n",
    "        self.post_pos_norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Transformer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, puzzle, x_t, t):\n",
    "        \"\"\"\n",
    "        puzzle, x_t: (batch, 81)\n",
    "        t: (batch,) integer steps in [1..max_T], describing which noising step\n",
    "        \"\"\"\n",
    "        batch_size = puzzle.size(0)\n",
    "        \n",
    "        # [puzzle || x_t] => shape (batch, 162)\n",
    "        inp = torch.cat([puzzle, x_t], dim=1)  # (batch, 162)\n",
    "        \n",
    "        # Token embeddings for puzzle + x_t\n",
    "        emb = self.embedding(inp)  # => (batch, 162, embed_dim)\n",
    "        \n",
    "        # 2) Time embedding => (batch, embed_dim)\n",
    "        t_emb = self.time_embedding(t)\n",
    "        # Expand to (batch, 1, embed_dim) so we can treat it like one extra token\n",
    "        t_emb = t_emb.unsqueeze(1)\n",
    "        \n",
    "        # 3) Concatenate time token with the puzzle+solution tokens => shape (batch, 163, embed_dim)\n",
    "        cat_emb = torch.cat([t_emb, emb], dim=1)\n",
    "        \n",
    "        # Add positional embeddings\n",
    "        cat_emb = cat_emb + self.pos_embedding\n",
    "        \n",
    "        # Apply layer normalization after positional embeddings\n",
    "        cat_emb = self.post_pos_norm(cat_emb)\n",
    "        \n",
    "        # Pass through Transformer\n",
    "        enc_out = self.encoder(cat_emb)  # => (batch, 163, embed_dim)\n",
    "        \n",
    "        # Output layer => (batch, 163, vocab_size)\n",
    "        logits = self.output_layer(enc_out)\n",
    "        \n",
    "        # The first token is the time token; the next 162 correspond to puzzle+solution\n",
    "        # Typically, we only care about the solution half => indices [1 + 81..]\n",
    "        # That becomes logits[:, (1+81):, :] if you want the solution part only\n",
    "        # But you can also just return the entire distribution:\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# 4) Diffusion train step\n",
    "# ------------------------------------------------------------------------\n",
    "def compute_subgoal_weights(puzzle, solution, vocab_size=10):\n",
    "    \"\"\"\n",
    "    Computes a difficulty-based weight for each cell in the puzzle:\n",
    "      - If puzzle[i] != 0, it's a given => typically weight = 0 or minimal (since it's known).\n",
    "      - If puzzle[i] == 0, compute how many digits (1..9) are valid in that cell\n",
    "        given Sudoku constraints. The more valid candidates => the harder the subgoal => higher weight.\n",
    "\n",
    "    Returns:\n",
    "        weights: A tensor of shape (batch, 81) with floating-point weights.\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = puzzle.size(0)\n",
    "    weights = torch.zeros_like(solution, dtype=torch.float32)\n",
    "\n",
    "    # We'll iterate over each puzzle in the batch\n",
    "    for b in range(batch_size):\n",
    "        # puzzle[b]: shape (81,)\n",
    "        # solution[b]: shape (81,)\n",
    "\n",
    "        # Convert puzzle[b] into a 9x9 grid for easier row/col/box indexing\n",
    "        puzzle_grid = puzzle[b].view(9, 9).cpu().numpy()  # shape (9,9), on CPU\n",
    "        # We'll also want to compute constraints for each row, col, box\n",
    "        # but let's do it cell-by-cell.\n",
    "\n",
    "        for idx in range(81):\n",
    "            r = idx // 9  # row\n",
    "            c = idx % 9   # column\n",
    "\n",
    "            if puzzle_grid[r, c] != 0:\n",
    "                # It's a given => we can optionally set weight to 0 or a small value\n",
    "                weights[b, idx] = 0.0\n",
    "            else:\n",
    "                # It's blank => compute how many valid digits remain\n",
    "                row_vals = set(puzzle_grid[r, :].tolist())\n",
    "                col_vals = set(puzzle_grid[:, c].tolist())\n",
    "\n",
    "                # Identify which 3x3 box (by top-left corner)\n",
    "                box_row = (r // 3) * 3\n",
    "                box_col = (c // 3) * 3\n",
    "                box_vals = set(\n",
    "                    puzzle_grid[box_row:box_row+3, box_col:box_col+3].reshape(-1).tolist()\n",
    "                )\n",
    "\n",
    "                # Givens can be 1..9, ignoring 0 (blank)\n",
    "                used_vals = (row_vals | col_vals | box_vals) - {0}\n",
    "                # valid digits are those in [1..9] not in used_vals\n",
    "                all_digits = set(range(1, vocab_size))  # {1,2,...,9} if vocab=10\n",
    "                valid_candidates = all_digits - used_vals\n",
    "\n",
    "                num_candidates = len(valid_candidates)\n",
    "\n",
    "                # Weight logic:\n",
    "                # e.g., let weight = num_candidates\n",
    "                # or weight = 1 + num_candidates, or scale by some factor\n",
    "                # The bigger the number of candidates => the bigger the weight\n",
    "                weights[b, idx] = float(num_candidates)\n",
    "\n",
    "    # Optionally normalize weights per puzzle\n",
    "    weights = weights / (weights.max(dim=1, keepdim=True)[0].clamp(min=1.0) + 1e-8)\n",
    "\n",
    "    return weights\n",
    "\n",
    "def diffusion_train_step(\n",
    "    model,\n",
    "    puzzle,\n",
    "    solution,\n",
    "    schedule,\n",
    "    optimizer,\n",
    "    vocab_size,\n",
    "    device,\n",
    "    loss_fn\n",
    "):\n",
    "    # Move puzzle and solution to GPU (or CPU if that's your chosen device)\n",
    "    puzzle = puzzle.to(device)\n",
    "    solution = solution.to(device)\n",
    "\n",
    "    T = schedule.T\n",
    "    t_int = np.random.randint(1, T + 1)          # pick a random diffusion step\n",
    "    t_tensor = torch.tensor([t_int] * puzzle.size(0), device=device)\n",
    "    \n",
    "    x_t = forward_diffusion_with_puzzle(\n",
    "        puzzle, solution, t_int, schedule, vocab_size, device\n",
    "    )\n",
    "    \n",
    "    # Pass t_tensor to model\n",
    "    logits = model(puzzle, x_t, t_tensor)   # <--- fix here\n",
    "    \n",
    "    # Slice out the solution portion if you added a time token\n",
    "    # The new shape is (batch, 163, vocab_size) if puzzle=81, x_t=81, plus 1 time token\n",
    "    # So the solution part is indices [82..162]\n",
    "    logits_solution_part = logits[:, 82:, :]\n",
    "    \n",
    "    ce_loss = loss_fn(\n",
    "        logits_solution_part.reshape(-1, vocab_size),\n",
    "        solution.reshape(-1)\n",
    "    )\n",
    "    \n",
    "    # Compute subgoal weights\n",
    "    weights = compute_subgoal_weights(puzzle, solution, vocab_size).to(device)\n",
    "    # Flatten to match (batch*81,)\n",
    "    weights_flat = weights.reshape(-1)\n",
    "\n",
    "    # Compute per-token NLL without detaching\n",
    "    nll_per_token = torch.nn.functional.cross_entropy(\n",
    "        logits_solution_part.reshape(-1, vocab_size),\n",
    "        solution.reshape(-1),\n",
    "        reduction='none'\n",
    "    )\n",
    "\n",
    "    # Apply weights\n",
    "    weighted_nll = nll_per_token * weights_flat\n",
    "\n",
    "    # Compute weighted loss\n",
    "    weighted_loss = weighted_nll.mean()\n",
    "\n",
    "    return weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# 5) Validation step\n",
    "# ------------------------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def diffusion_eval_step(\n",
    "    model, \n",
    "    puzzle,\n",
    "    solution,\n",
    "    schedule,\n",
    "    loss_fn,\n",
    "    vocab_size,\n",
    "    device\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs a forward pass during evaluation with time conditioning.\n",
    "\n",
    "    Args:\n",
    "        model: The PuzzleDenoiser model with time conditioning.\n",
    "        puzzle: Tensor of shape (batch, 81) with puzzle digits (0 for blanks).\n",
    "        solution: Tensor of shape (batch, 81) with solution digits.\n",
    "        schedule: The diffusion schedule object containing alphas and T.\n",
    "        loss_fn: The loss function, e.g., nn.CrossEntropyLoss().\n",
    "        vocab_size: Size of the vocabulary (digits 0-9 => 10).\n",
    "        device: torch.device to perform computations on.\n",
    "\n",
    "    Returns:\n",
    "        loss: The evaluation loss as a float.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    puzzle = puzzle.to(device)\n",
    "    solution = solution.to(device)\n",
    "\n",
    "    # 1. Sample a random diffusion step t for the entire batch\n",
    "    T = schedule.T\n",
    "    t_int = np.random.randint(1, T + 1)  # Sample t in [1, T]\n",
    "    # Create a tensor of shape (batch_size,) filled with t_int\n",
    "    t_tensor = torch.full((puzzle.size(0),), t_int, dtype=torch.long, device=device)\n",
    "\n",
    "    # 2. Create x_t with forward diffusion\n",
    "    x_t = forward_diffusion_with_puzzle(\n",
    "        puzzle, solution, t_int, schedule, vocab_size, device\n",
    "    )\n",
    "\n",
    "    # 3. Forward pass with time conditioning\n",
    "    logits = model(puzzle, x_t, t_tensor)  # shape: (batch, 163, vocab_size)\n",
    "\n",
    "    # 4. Slice out the solution part\n",
    "    # Assuming time token is at position 0:\n",
    "    # - Indices 1-81: puzzle tokens\n",
    "    # - Indices 82-162: x_t tokens\n",
    "    logits_solution_part = logits[:, 82:, :]  # shape: (batch, 81, vocab_size)\n",
    "\n",
    "    # 5. Compute cross-entropy loss\n",
    "    loss = loss_fn(\n",
    "        logits_solution_part.reshape(-1, vocab_size),  # (batch*81, vocab_size)\n",
    "        solution.reshape(-1)                           # (batch*81,)\n",
    "    )\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# 6) Iterative decoding to fill blank cells\n",
    "# ------------------------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def iterative_decode(\n",
    "    model,\n",
    "    puzzle,        # shape (batch, 81) puzzle givens\n",
    "    schedule,\n",
    "    vocab_size,\n",
    "    device,\n",
    "    steps=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Iteratively decodes the puzzle by denoising from t=T to t=1 with time conditioning.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    puzzle = puzzle.to(device)\n",
    "    batch_size, seq_len = puzzle.shape\n",
    "    T = schedule.T if steps is None else steps\n",
    "\n",
    "    # Initialize x_t: noise only in blank positions\n",
    "    x_t = puzzle.clone()\n",
    "    blank_mask = (puzzle == 0)\n",
    "    x_t[blank_mask] = torch.randint(0, vocab_size, x_t[blank_mask].shape, device=device)\n",
    "\n",
    "    for curr_t in range(T, 0, -1):\n",
    "        # Create a tensor of the current step, repeated for the batch\n",
    "        t_tensor = torch.full((batch_size,), curr_t, dtype=torch.long, device=device)\n",
    "\n",
    "        # Forward pass with time conditioning\n",
    "        logits = model(puzzle, x_t, t_tensor)  # shape: (batch, 163, vocab_size)\n",
    "\n",
    "        # Slice out the solution part\n",
    "        logits_solution_part = logits[:, 82:, :]  # shape: (batch, 81, vocab_size)\n",
    "\n",
    "        # Predict the solution tokens\n",
    "        pred_sol = logits_solution_part.argmax(dim=-1)  # shape: (batch, 81)\n",
    "\n",
    "        # Update only blank cells\n",
    "        x_t[blank_mask] = pred_sol[blank_mask]\n",
    "\n",
    "    return x_t  # Final filled solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate_combined(\n",
    "    model, \n",
    "    schedule,\n",
    "    loader,        # A DataLoader for validation\n",
    "    device,\n",
    "    loss_fn,\n",
    "    vocab_size=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Combines validation loss, solve rate, and token accuracy in one pass.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    solved_count = 0\n",
    "    correct_token_count = 0\n",
    "    total_token_count = 0\n",
    "\n",
    "    with torch.cuda.amp.autocast():\n",
    "        for puzzles, solutions in tqdm(loader, desc=\"Validation\"):\n",
    "            puzzles = puzzles.to(device)\n",
    "            solutions = solutions.to(device)\n",
    "            batch_size = puzzles.size(0)\n",
    "\n",
    "            # 1) Compute a random t for the entire batch and get x_t\n",
    "            t_int = np.random.randint(1, schedule.T + 1)\n",
    "            t_tensor = torch.full((batch_size,), t_int, dtype=torch.long, device=device)\n",
    "\n",
    "            x_t = forward_diffusion_with_puzzle(\n",
    "                puzzle=puzzles,\n",
    "                solution=solutions,\n",
    "                t=t_int,\n",
    "                schedule=schedule,\n",
    "                vocab_size=vocab_size,\n",
    "                device=device\n",
    "            )\n",
    "            \n",
    "            # 2) Forward pass with time conditioning\n",
    "            logits = model(puzzles, x_t, t_tensor)                  # shape (batch, 163, vocab_size)\n",
    "            logits_solution_part = logits[:, 82:, :]                # only solution part\n",
    "            \n",
    "            # 3) Compute cross-entropy loss\n",
    "            loss = loss_fn(\n",
    "                logits_solution_part.reshape(-1, vocab_size),\n",
    "                solutions.reshape(-1)\n",
    "            )\n",
    "            \n",
    "            total_loss += loss.item() * batch_size\n",
    "            total_samples += batch_size\n",
    "\n",
    "            # 4) Iterative decode to get final filled boards\n",
    "            x_filled = iterative_decode(\n",
    "                model=model,\n",
    "                puzzle=puzzles,\n",
    "                schedule=schedule,\n",
    "                vocab_size=vocab_size,\n",
    "                device=device\n",
    "            )\n",
    "            \n",
    "            # 5) Solve rate: how many boards are 100% correct\n",
    "            eq_mask = (x_filled == solutions)          # (batch, 81)\n",
    "            batch_solved = eq_mask.all(dim=1).sum().item()\n",
    "            solved_count += batch_solved\n",
    "\n",
    "            # 6) Token accuracy on masked cells\n",
    "            masked_mask = (puzzles == 0)               # (batch, 81)\n",
    "            correct_masked = eq_mask & masked_mask      # Correct predictions on masked cells\n",
    "            correct_token_count += correct_masked.sum().item()\n",
    "            total_token_count += masked_mask.sum().item()\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    solve_rate = solved_count / total_samples\n",
    "    token_acc = correct_token_count / total_token_count if total_token_count > 0 else 0.0\n",
    "\n",
    "    return avg_loss, solve_rate, token_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# 7) Putting it all together: example training loop\n",
    "# ------------------------------------------------------------------------\n",
    "def train_puzzle_diffusion(\n",
    "    X_train, y_train, \n",
    "    X_val,   y_val,\n",
    "    vocab_size=10,\n",
    "    T=40,\n",
    "    embed_dim=512,\n",
    "    num_heads=8,\n",
    "    num_layers=8,\n",
    "    batch_size=64,\n",
    "    num_epochs=50,\n",
    "    best_model_path=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains the PuzzleDenoiser model with time-conditioned diffusion.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    schedule = SigmoidDiffusionSchedule(\n",
    "        T=T, \n",
    "        min_alpha=0.1, \n",
    "        max_alpha=0.7,\n",
    "        k=12.0\n",
    "    )\n",
    "\n",
    "    # Handle multi-GPU setup\n",
    "    multi_gpu = torch.cuda.device_count() > 1\n",
    "    if multi_gpu:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    else:\n",
    "        print(\"Using 1 GPU or CPU.\")\n",
    "\n",
    "    # Initialize the model with max_T matching the schedule\n",
    "    model = PuzzleDenoiser(\n",
    "        vocab_size=vocab_size, \n",
    "        embed_dim=embed_dim, \n",
    "        num_heads=num_heads, \n",
    "        num_layers=num_layers,\n",
    "        max_T=T\n",
    "    ).to(device)\n",
    "\n",
    "    # Load best model if provided (handling DataParallel)\n",
    "    if best_model_path is not None:\n",
    "        state_dict = torch.load(best_model_path, map_location=device)\n",
    "        \n",
    "        if multi_gpu and not list(state_dict.keys())[0].startswith('module.'):\n",
    "            model.load_state_dict(state_dict)\n",
    "            model = nn.DataParallel(model)\n",
    "        elif not multi_gpu and list(state_dict.keys())[0].startswith('module.'):\n",
    "            new_state_dict = {k[7:]: v for k, v in state_dict.items()}\n",
    "            model.load_state_dict(new_state_dict)\n",
    "        else:\n",
    "            model.load_state_dict(state_dict)\n",
    "            \n",
    "        print(f\"Loaded model from {best_model_path}\")\n",
    "    elif multi_gpu:\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # Setup data loaders\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset   = TensorDataset(X_val,   y_val)\n",
    "    train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader    = DataLoader(val_dataset,   batch_size=batch_size)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    # Initial validation before training\n",
    "    if False:\n",
    "        print(\"Running initial validation (Epoch 0):\")\n",
    "        avg_val_loss, val_solve_rate, val_token_acc = validate_combined(\n",
    "            model=model,\n",
    "            schedule=schedule,\n",
    "            loader=val_loader,\n",
    "            device=device,\n",
    "            loss_fn=loss_fn,\n",
    "            vocab_size=vocab_size\n",
    "        )\n",
    "        print(f\"Initial val_loss={avg_val_loss:.4f}\")\n",
    "        print(f\"Initial solve rate on validation set: {val_solve_rate*100:.2f}%\")\n",
    "        print(f\"Initial token-level accuracy on validation set: {val_token_acc*100:.2f}%\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch_puzzle, batch_solution in tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\"):\n",
    "            # Use autocast for mixed precision training\n",
    "            with torch.cuda.amp.autocast():\n",
    "                loss_val = diffusion_train_step(\n",
    "                    model=model,\n",
    "                    puzzle=batch_puzzle,\n",
    "                    solution=batch_solution,\n",
    "                    schedule=schedule,\n",
    "                    optimizer=optimizer,\n",
    "                    loss_fn=loss_fn,\n",
    "                    vocab_size=vocab_size,\n",
    "                    device=device\n",
    "                )\n",
    "            \n",
    "            # Scale loss and do backward pass\n",
    "            scaler.scale(loss_val).backward()\n",
    "            \n",
    "            # Unscale gradients and optimizer step\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            train_losses.append(loss_val.item())\n",
    "            \n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "\n",
    "        # Validation\n",
    "        avg_val_loss, val_solve_rate, val_token_acc = validate_combined(\n",
    "            model=model,\n",
    "            schedule=schedule,\n",
    "            loader=val_loader,\n",
    "            device=device,\n",
    "            loss_fn=loss_fn,\n",
    "            vocab_size=vocab_size\n",
    "        )\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: train_loss={avg_train_loss:.4f}, val_loss={avg_val_loss:.4f}\")\n",
    "        print(f\"Solve rate on validation set: {val_solve_rate*100:.2f}%\")\n",
    "        print(f\"Token-level accuracy on validation set: {val_token_acc*100:.2f}%\")\n",
    "\n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), \"puzzle_diffuser_best.pt\")\n",
    "            print(f\"  [*] Best model saved @ val_loss={avg_val_loss:.4f}\")\n",
    "\n",
    "    return model, schedule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load sudoku data\n",
    "df = pd.read_csv('./data/sudoku.csv')\n",
    "\n",
    "# Convert strings to tensors\n",
    "def preprocess_sudoku(puzzle_str):\n",
    "    # Convert string to list of integers and then to tensor\n",
    "    return torch.tensor([int(d) for d in puzzle_str], dtype=torch.long)\n",
    "\n",
    "# Convert all puzzles and solutions\n",
    "puzzles = torch.stack([preprocess_sudoku(p) for p in df['quizzes']])\n",
    "solutions = torch.stack([preprocess_sudoku(s) for s in df['solutions']])\n",
    "\n",
    "# Karpathy split (90/5/5)\n",
    "train_size = 0.9\n",
    "val_size = 0.05\n",
    "test_size = 0.05\n",
    "\n",
    "# First split into train and temp\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    puzzles, solutions, train_size=train_size, random_state=42\n",
    ")\n",
    "\n",
    "# Split temp into val and test\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using 2 GPUs!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad6b08cade248b89163e23ca9b0b463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 [Train]:   0%|          | 0/14063 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, schedule \u001b[38;5;241m=\u001b[39m train_puzzle_diffusion(\n\u001b[1;32m      2\u001b[0m     X_train, y_train,\n\u001b[1;32m      3\u001b[0m     X_val,   y_val,\n\u001b[1;32m      4\u001b[0m     vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m      5\u001b[0m     T\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m,\n\u001b[1;32m      6\u001b[0m     embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m      7\u001b[0m     num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m      8\u001b[0m     num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m      9\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m     10\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m#best_model_path=\"puzzle_diffuser_best.pt\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m )\n",
      "Cell \u001b[0;32mIn[9], line 95\u001b[0m, in \u001b[0;36mtrain_puzzle_diffusion\u001b[0;34m(X_train, y_train, X_val, y_val, vocab_size, T, embed_dim, num_heads, num_layers, batch_size, num_epochs, best_model_path)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_puzzle, batch_solution \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m [Train]\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m# Use autocast for mixed precision training\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[0;32m---> 95\u001b[0m         loss_val \u001b[38;5;241m=\u001b[39m diffusion_train_step(\n\u001b[1;32m     96\u001b[0m             model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     97\u001b[0m             puzzle\u001b[38;5;241m=\u001b[39mbatch_puzzle,\n\u001b[1;32m     98\u001b[0m             solution\u001b[38;5;241m=\u001b[39mbatch_solution,\n\u001b[1;32m     99\u001b[0m             schedule\u001b[38;5;241m=\u001b[39mschedule,\n\u001b[1;32m    100\u001b[0m             optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m    101\u001b[0m             loss_fn\u001b[38;5;241m=\u001b[39mloss_fn,\n\u001b[1;32m    102\u001b[0m             vocab_size\u001b[38;5;241m=\u001b[39mvocab_size,\n\u001b[1;32m    103\u001b[0m             device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m    104\u001b[0m         )\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# Scale loss and do backward pass\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mscale(loss_val)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[5], line 102\u001b[0m, in \u001b[0;36mdiffusion_train_step\u001b[0;34m(model, puzzle, solution, schedule, optimizer, vocab_size, device, loss_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m ce_loss \u001b[38;5;241m=\u001b[39m loss_fn(\n\u001b[1;32m     97\u001b[0m     logits_solution_part\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size),\n\u001b[1;32m     98\u001b[0m     solution\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     99\u001b[0m )\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Compute subgoal weights\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m weights \u001b[38;5;241m=\u001b[39m compute_subgoal_weights(puzzle, solution, vocab_size)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Flatten to match (batch*81,)\u001b[39;00m\n\u001b[1;32m    104\u001b[0m weights_flat \u001b[38;5;241m=\u001b[39m weights\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 59\u001b[0m, in \u001b[0;36mcompute_subgoal_weights\u001b[0;34m(puzzle, solution, vocab_size)\u001b[0m\n\u001b[1;32m     53\u001b[0m             num_candidates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(valid_candidates)\n\u001b[1;32m     55\u001b[0m             \u001b[38;5;66;03m# Weight logic:\u001b[39;00m\n\u001b[1;32m     56\u001b[0m             \u001b[38;5;66;03m# e.g., let weight = num_candidates\u001b[39;00m\n\u001b[1;32m     57\u001b[0m             \u001b[38;5;66;03m# or weight = 1 + num_candidates, or scale by some factor\u001b[39;00m\n\u001b[1;32m     58\u001b[0m             \u001b[38;5;66;03m# The bigger the number of candidates => the bigger the weight\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m             weights[b, idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(num_candidates)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Optionally normalize weights per puzzle\u001b[39;00m\n\u001b[1;32m     62\u001b[0m weights \u001b[38;5;241m=\u001b[39m weights \u001b[38;5;241m/\u001b[39m (weights\u001b[38;5;241m.\u001b[39mmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, schedule = train_puzzle_diffusion(\n",
    "    X_train, y_train,\n",
    "    X_val,   y_val,\n",
    "    vocab_size=10,\n",
    "    T=40,\n",
    "    embed_dim=512,\n",
    "    num_heads=8,\n",
    "    num_layers=8,\n",
    "    batch_size=64,\n",
    "    num_epochs=50\n",
    "    #best_model_path=\"puzzle_diffuser_best.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 2) CREATE THE NEW SCHEDULE\n",
    "#############################################\n",
    "schedule = SigmoidDiffusionSchedule(\n",
    "    T=30,\n",
    "    min_alpha=0.1,\n",
    "    max_alpha=0.7,\n",
    "    k=12.0\n",
    ")\n",
    "\n",
    "#############################################\n",
    "# 3) PICK A SAMPLE PUZZLE & SOLUTION\n",
    "#############################################\n",
    "sample_puzzle = X_train[0:1]\n",
    "sample_solution = y_train[0:1]\n",
    "\n",
    "print(\"Puzzle:\", sample_puzzle)\n",
    "print(\"Solution:\", sample_solution)\n",
    "\n",
    "#############################################\n",
    "# 4) SHOW x_1, x_2, ... x_T\n",
    "#############################################\n",
    "vocab_size = 10  # digits 0-9\n",
    "for t in range(1, schedule.T+1):\n",
    "    alpha_t = schedule[t]  # just to see the noise fraction\n",
    "    x_t = forward_diffusion_with_puzzle(\n",
    "        puzzle=sample_puzzle, \n",
    "        solution=sample_solution, \n",
    "        t=t, \n",
    "        schedule=schedule, \n",
    "        vocab_size=vocab_size, \n",
    "        device=device\n",
    "    )\n",
    "    print(f\"x_{t} (alpha={alpha_t:.3f}) =\", x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Then to do iterative decoding:\n",
    "puzzle_batch = X_val[0:2]  # for example\n",
    "x_filled = iterative_decode(\n",
    "    model, \n",
    "    puzzle=puzzle_batch, \n",
    "    schedule=schedule, \n",
    "    vocab_size=10,\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    ")\n",
    "print(\"Puzzle: \", puzzle_batch)\n",
    "print(\"Decoded solution: \", x_filled)\n",
    "print(\"Original solution: \", y_val[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = measure_sudoku_solve_rate(\n",
    "    model,\n",
    "    schedule,\n",
    "    X_val[0:100],\n",
    "    y_val[0:100],\n",
    "    device=device,\n",
    "    vocab_size=10,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "print(f\"Solve rate on validation set: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small validation dataset for testing\n",
    "X_val_test = X_val[:1000]\n",
    "y_val_test = y_val[:1000]\n",
    "test_val_loader = DataLoader(\n",
    "    TensorDataset(X_val_test, y_val_test),\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Initialize loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Run validation\n",
    "avg_val_loss, val_solve_rate, val_token_acc = validate_combined(\n",
    "    model=model,\n",
    "    schedule=schedule, \n",
    "    loader=test_val_loader,\n",
    "    device=device,\n",
    "    loss_fn=loss_fn,\n",
    "    vocab_size=vocab_size\n",
    ")\n",
    "\n",
    "print(f\"Validation loss: {avg_val_loss:.4f}\")\n",
    "print(f\"Solve rate on validation set: {val_solve_rate*100:.2f}%\")\n",
    "print(f\"Token-level accuracy on validation set: {val_token_acc*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
